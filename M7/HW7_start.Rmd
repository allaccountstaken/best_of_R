---
title: "Homework7"
author: "Dima Mikhaylov"
date: "10/21/2021"
output: html_document
---

## 1. Questions based on `swiss` dataset:

```{r}
library(datasets)
library(tidyverse)
library(corrplot)
library(GGally)
data(swiss)
head(swiss)
```

a. In the previous homework, you fit a model with the fertility measure as the response variable and used all the other variables as predictors. Now, consider a simpler model, using only the last three variables as predictors: Education, Catholic, and Infant.Mortality. Carry out an appropriate hypothesis test to assess which of these two models should be used. State the null and alternative hypotheses, find the relevant test statistic, p-value, and state a conclusion in context. (For practice, try to calculate the test statistic by hand.)

#### Below is the original full model:

```{r}
full_lm.fit <- lm(Fertility ~ ., data=swiss)
summary(full_lm.fit)
```

##### This is reduced subset model:

```{r}
reduced_lm.fit <- lm(Fertility ~ Education + Catholic + Infant.Mortality, data=swiss)
summary(reduced_lm.fit)
```

#### In order to compare these two models one needs to test H0: B(Agriculture) = B(Examination) = 0 and alternative Ha: B(Agriculture) and B(Examination) together are not 0. Comparing these two models with partial F test below:

```{r}
anova(reduced_lm.fit, full_lm.fit)
```

#### Based on these results p-value is not statistically significant at 5% and thus we fail to reject H0. In other words, the data suggests that we can drop variables Agriculture and Examinbation. Below is additional calculations "by hand":

```{r}
test_lm.fit <- lm(Fertility ~ Education + Catholic + Infant.Mortality + ., data=swiss)
anova(test_lm.fit)
```

#### Checking Sum of Squares and F-stat:

```{r}
F0_stat = ((264.2+53)/2)/(2105/41)
F0_stat
```

#### Checking significance of the F-stat by computing p-value:

```{r}
1-pf(F0_stat, 2, 41)
```

#### Conclusion: based on these manual calculations one arrives to the similar conclusion - failing to reject H0 at 95% significance and therefore Agriculture and Examinbation can be dropped from the model.


b. For the model you decide to use from part 1a, assess if the regression assumptions are met.

#### Overall, linear regression model assumptions are met.

```{r}
par(mfrow=c(2,2))
plot(reduced_lm.fit)
```

#### Focusing on the Residuals plot, variances seems to be constant and overall mean tends to be zero. Moreover, from the QQ plot, theoretical quantiles were matched. Standartized residuals are well distributed and there are not to many high leverage points (still note presence of leverage from "Rive Gauche", "Moutier", and "Sierre" points)


## 2. Questions based on the data from 113 hospitals.


a. Based on the t statistics, which predictors appear to be insignificant?

#### Based on the t statistics only "Stay" and "Culture" slopes are significant. The other three predictors, "Age", "Census", and "Beds" appear to be insignificant (given the presence of the other predictors).

b. Based on your answer in part 2a, carry out the appropriate hypothesis test to see if those predictors can be dropped from the multiple regression model. Show all steps, including your null and alternative hypotheses, the corresponding test statistic, p-value, critical value, and your conclusion in context.

#### HO: B("Age") = B("Census") = B("Beds") = 0 and Ha: B("Age"), B("Census"),  B("Beds") not 0, given the presence of the other predictors.

#### Compute partial F statistic:

```{r}
F_stat = ((0.136 + 5.101 + 0.028) / 3) / (105.413 / 107)
F_stat
```

#### Compute p-value of F statistic:

```{r}
1-pf(F_stat, 3, 107)
```

#### Based on these results, we fail to reject H0 at 95% significance and thus we can drop these predictors from the model.


c. Suppose we want to decide between two potential models:
  * Model 1: using x1, x2, x3, x4 as the predictors for InfctRsk
  * Model 2: using x1, x2 as the predictors for InfctRsk
Carry out the appropriate hypothesis test to decide which of models 1 or 2 should be used. Be sure to show all steps in your hypothesis test.

#### Does removing x3 and x4 from the full model (`InfctRsk~x1+x2+x3+x4`) result in a statistically significant increase in the sum of squared errors, i.e. residual sum of squares? In other words, does removing these two variables result in increased error and thus decreased predictive power of the model? All esle equal, simplier model should always be preferable.


#### H0: there is no difference in SSE of full and reduced models, i.e. models do not significantly differ.
#### Ha: full model (`InfctRsk~x1+x2+x3+x4`) has significantly lower SSE than the reduced model(`InfctRsk~x1+x2`)

#### F-stat - (SSres(reduced)-SSres(full)/(chage in # of params)) / MSres(full)

  * SSr(x1) = 57.305
  * SSr(x2|x1) = 33.397
  * SSr(x3|x1, x2) = 0.136
  * SSr(x4|x1, x2, x3) = 5.101

```{r}
Original_SSt <- 57.305 + 33.397 + 0.136 +  5.101 + 0.028 + 105.413 # Total sum of squared from 5-predictor model

Model2_SSr <- 57.305 + 33.397 # reduced Model 2 sum or squares, explained by the reduced subset
Model1_SSr <- 57.305 + 33.397 + 0.136 +  5.101 # full Model 1 sum or squares, explained by the full model

Model2_SSres <- Original_SSt - Model2_SSr # Res. - variations not explained by Model 2, the reduced subset
Model1_SSres <- Original_SSt  - Model1_SSr # Res. - variations not explained by Model 1, the full model

#F_stat = ((Model2_SSres-Model1_SSres)/2) / ((0.028 + 105.413)/107) #Alternative formulae
F_stat = ((Model1_SSr-Model2_SSr)/2) / ((0.028 + 105.413)/107)

1-pf(F_stat, 2, 107)

```

#### Concslusion: p-value is high and thus fail to reject H0 at 95% significance, there is no difference in SSE of full and reduced models, and thus we can drop these predictors from the model. In other words, epthasising simplicity we should fo with Model 2: using x1, x2 as the predictors for InfctRsk


## 3. Questions based on a data set seen in Homework Set 4.

Explain how this output indicates the presence of multicollinearity in this regression
model.


#### The output suggests the presence of collinearity based on the following tests:

  * The individual t tests suggests none of the two predictors are significant (given the presence of
the other predictors)
  * The F test suggests that overall the model is useful in predicting the response because p-value of F-stats is very small. 
  * Business logic suggests that the two predictors are strongly correlated and thus removing one of them or combining both into one variable will help increase significance of t statistics. 


